{"cells":[{"cell_type":"markdown","metadata":{"trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8C68EF885E654EF79954B60A23767CC2","runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"6836f6e35479c3a76ac12206"},"source":"# 数据处理"},{"cell_type":"code","metadata":{"id":"2EFCEBC49FC0429BB2B47B5171E2E916","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"import pandas as pd\n\n# 文件路径列表\nfile_names = [\n    '/home/mw/input/dataset7967/Health_20_0.csv',\n    '/home/mw/input/dataset7967/Chipped_20_0.csv',\n    '/home/mw/input/dataset7967/Miss_20_0.csv',\n    '/home/mw/input/dataset7967/Root_20_0.csv',\n    '/home/mw/input/dataset7967/Surface_20_0.csv',\n    '/home/mw/input/dataset7967/ball_20_0.csv',  # 这个文件特殊处理\n    '/home/mw/input/dataset7967/inner_20_0.csv',\n    '/home/mw/input/dataset7967/outer_20_0.csv'\n]\n\n# 列名列表\ncolumns_name = [\n    'Health_20_0', 'Chipped_20_0', 'Miss_20_0', 'Root_20_0',\n    'Surface_20_0', 'ball_20_0', 'inner_20_0', 'outer_20_0'\n]\n\n# 初始化空的 DataFrame\ndata_1 = pd.DataFrame()\n\n# 遍历文件列表\nfor index in range(len(file_names)):\n    try:\n        # 读取 CSV 文件，假设文件是以制表符分隔的\n        data = pd.read_csv(file_names[index], sep=\"\\t\", header=None, low_memory=False)\n        \n        # 提取 16-103440 行的数据\n        data1 = data.iloc[1015:202063]\n        \n        # 只保留前 8 列\n        data2 = data1.iloc[:, :8]\n        \n        # 提取第三列数据\n        data3 = data2.iloc[:, 2]\n        \n        # 将数据存储到 DataFrame 中\n        data_1[columns_name[index]] = data3.reset_index(drop=True)[:201048]\n    \n    except Exception as e:\n        # **如果是 ball_20_0.csv，则单独处理**\n        if \"ball_20_0.csv\" in file_names[index]:\n            try:\n                data = pd.read_csv(file_names[index], header=None, low_memory=False)\n\n                # 提取 16-103440 行的数据\n                data1 = data.iloc[1015:202063]\n\n                # 只保留前 8 列\n                data2 = data1.iloc[:, :8]\n\n                # 确保 data2 至少有 3 列\n                if data2.shape[1] >= 3:\n                    data3 = data2.iloc[:, 2]\n                    data_1[\"ball_20_0\"] = data3.reset_index(drop=True)[:201048]\n\n            except Exception as e:\n                print(f\"Failed to process ball_20_0.csv manually: {e}\")\n\n# 保存结果到 CSV 文件\ndata_1.to_csv('data_1.csv', index=False)\n","outputs":[],"execution_count":1},{"cell_type":"markdown","metadata":{"id":"51ACB1C106CB472B8489EAFD48EA0F92","notebookId":"6836f6e35479c3a76ac12206","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 数据标准化及分割"},{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"082AB3242E044E27B524ED8BBFF26F5A","scrolled":false,"notebookId":"6836f6e35479c3a76ac12206"},"source":"import numpy as np\nimport pandas as pd\n\ndata_array = data_1.values\n\n# 窗口大小和步长\nwindow_size = 2048\nstep_size = 1000\n\n# 存储分割后的数据\nsegmented_data = []\nlabels = []\n\n# 遍历每个类别的列，进行窗口滑动分割\nfor label, column_index in enumerate(range(data_array.shape[1])):\n    column_data = data_array[:, column_index]  # 获取当前类别的数据\n    num_samples = (len(column_data) - window_size) // step_size + 1  # 计算可分割的样本数\n\n    for i in range(num_samples):\n        start = i * step_size\n        end = start + window_size\n        segmented_data.append(column_data[start:end])  # 提取窗口数据\n        labels.append(label)  # 记录对应的类别\n\n# 转换为 NumPy 数组\nsegmented_data = np.array(segmented_data)\nlabels = np.array(labels)\n\n# 将数据保存为 DataFrame\ndata_2 = pd.DataFrame(segmented_data)\ndata_2['labels'] = labels  # 添加标签列\n\n# 保存为 CSV 文件\ndata_2.to_csv('data_2.csv', index=False)\ndata_2.shape","outputs":[{"output_type":"execute_result","data":{"text/plain":"(1600, 2049)"},"metadata":{},"execution_count":3}],"execution_count":3},{"cell_type":"markdown","metadata":{"id":"2B724D92B8ED49868D2D9C7FF18F8C26","notebookId":"6836f6e35479c3a76ac12206","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 浅层学习部分的数据"},{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0EE5F68C0F534D7C97DF61F96BB2BAE1","scrolled":false,"notebookId":"6836f6e35479c3a76ac12206"},"source":"import pywt\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import kurtosis, skew\nfrom scipy.signal import welch\nfrom numpy.linalg import svd\n\n# --- 参数设定 ---\nwavelet = 'db4' # 小波基函数\nlevel = 4      # 分解层数\n\n\n# --- 特征提取函数 ---\ndef extract_time_features(reconstructed_signal, cA, cD):\n    \"\"\"\n    从重构信号计算时域统计特征，\n    从原始分解系数计算能量特征。\n    \"\"\"\n    reconstructed_signal = np.asarray(reconstructed_signal, dtype=np.float64)\n    cA = np.asarray(cA, dtype=np.float64)\n    cD = [np.asarray(d, dtype=np.float64) for d in cD]\n\n    # 基于重构信号的特征\n    max_val = np.max(reconstructed_signal)\n    min_val = np.min(reconstructed_signal)\n    mean_val = np.mean(reconstructed_signal)\n    peak_val = np.max(np.abs(reconstructed_signal))\n    peak_to_peak = max_val - min_val\n    kurt = kurtosis(reconstructed_signal)\n    skewness = skew(reconstructed_signal)\n    rms = np.sqrt(np.mean(reconstructed_signal**2))\n    rectified_mean = np.mean(np.abs(reconstructed_signal))\n    waveform_factor = rms / rectified_mean if rectified_mean != 0 else 0\n    peak_factor = peak_val / rms if rms != 0 else 0\n    sqrt_abs_mean = np.mean(np.sqrt(np.abs(reconstructed_signal)))\n    margin_factor = peak_val / (sqrt_abs_mean ** 2) if sqrt_abs_mean != 0 else 0\n\n    # 基于原始分解系数的能量特征\n    energy_high = sum(np.sum(d**2) for d in cD) # 高频细节系数能量\n    energy_low = np.sum(cA**2)                 # 低频近似系数能量\n\n    return [max_val, min_val, mean_val, peak_val, peak_to_peak, kurt, skewness, rms,\n            rectified_mean, waveform_factor, peak_factor, margin_factor, energy_high, energy_low]\n\n\ndef extract_frequency_features(reconstructed_signal, fs=1.0):\n    \"\"\"从重构信号计算频域特征 (Welch法)。\"\"\"\n    signal = np.asarray(reconstructed_signal, dtype=np.float64)\n    # 使用min(len(signal), 256)确保nperseg不超过信号长度\n    freqs, psd = welch(signal, fs=fs, nperseg=min(len(signal), 256))\n    psd = psd.astype(np.float64)\n\n    sum_psd = np.sum(psd)\n    if sum_psd == 0:\n        return [0, 0, 0, 0, 0] # 如果PSD全零，返回0\n\n    centroid_freq = np.sum(freqs * psd) / sum_psd\n    mean_square_freq = np.sum((freqs**2) * psd) / sum_psd\n    freq_variance = mean_square_freq - centroid_freq**2\n    freq_band_energy = sum_psd\n    freq_std = np.sqrt(freq_variance) if freq_variance >= 0 else 0\n\n    return [centroid_freq, mean_square_freq, freq_variance, freq_band_energy, freq_std]\n\n\ndef extract_svd_features(cA, cD):\n    \"\"\"从原始分解系数计算SVD特征。\"\"\"\n    cA = np.asarray(cA, dtype=np.float64).ravel()\n    cD_flat = [np.asarray(d, dtype=np.float64).ravel() for d in cD]\n    high_freq_components = np.concatenate(cD_flat) if cD_flat else np.array([], dtype=np.float64)\n\n    # 低频奇异值\n    low_freq_svd = 0\n    if cA.size > 0:\n        U_a, S_a, V_a = svd(cA.reshape(-1, 1), full_matrices=False)\n        if len(S_a) > 0:\n            low_freq_svd = S_a[0]\n\n    # 高频奇异值\n    high_freq_svd = 0\n    if high_freq_components.size > 0:\n        U_d, S_d, V_d = svd(high_freq_components.reshape(-1, 1), full_matrices=False)\n        if len(S_d) > 0:\n            high_freq_svd = S_d[0]\n\n    return [high_freq_svd, low_freq_svd]\n\n\n# --- 主处理流程 ---\nfeatures_matrix = []\n\nfor sample in segmented_data:\n    sample = np.asarray(sample, dtype=np.float64).ravel()\n\n    # 1. 小波分解\n    coeffs = pywt.wavedec(sample, wavelet, level=level)\n    cA = coeffs[0]\n    cD = coeffs[1:]\n\n    # 2. 小波重构\n    reconstructed_sample = pywt.waverec(coeffs, wavelet)\n    # 可选: 如果需要，截断/填充以匹配原始长度\n    # reconstructed_sample = reconstructed_sample[:len(sample)]\n\n    # 3. 提取特征\n    time_features = extract_time_features(reconstructed_sample, cA, cD) # 时域(重构信号)+能量(系数)\n    freq_features = extract_frequency_features(reconstructed_sample)     # 频域(重构信号)\n    svd_features = extract_svd_features(cA, cD)                         # SVD(系数)\n\n    # 合并特征\n    features_matrix.append(time_features + freq_features + svd_features)\n\n# --- 创建并保存DataFrame ---\nfeature_columns = [\n    \"最大值\", \"最小值\", \"平均值\", \"峰值\", \"峰峰值\", \"峭度\", \"偏度\", \"均方根\",\n    \"整流平均值\", \"波形因子\", \"峰值因子\", \"裕度因子\", \"高频能量\", \"低频能量\",\n    \"重心频率\", \"均方频率\", \"频率方差\", \"频带能量\", \"频率标准差\",\n    \"高频奇异值特征\", \"低频奇异值特征\"\n]\n\nfeatures_df = pd.DataFrame(features_matrix, columns=feature_columns)\nfeatures_df[\"故障类别\"] = labels # 直接添加标签列，假设长度匹配\n\n# 保存结果\nfeatures_df.to_csv(\n    'features_reconstructed_simplified_df.csv',\n    index=False,\n    encoding='utf-8-sig',\n    float_format='%.3f'  # <--- 添加此参数\n)\nprint(\"特征已提取并保存到 features_reconstructed_simplified_df.csv\")","outputs":[{"output_type":"stream","name":"stdout","text":"特征已提取并保存到 features_reconstructed_simplified_df.csv\n"}],"execution_count":4},{"cell_type":"markdown","metadata":{"id":"366B3305A2B14D328AFF0F206220AC02","notebookId":"6836f6e35479c3a76ac12206","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 随机森林"},{"cell_type":"code","metadata":{"id":"75D454ADA4014E7CA26801D434776E4D","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"from sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns # 用于绘图\n\n# 提取特征和标签\nX = features_df.iloc[:, :-1]  # 特征\ny = features_df.iloc[:, -1]   # 标签\n# 打乱数据\nfeatures_df = features_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# 训练随机森林进行特征重要性评估\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# 获取特征重要性\nfeature_importances = rf.feature_importances_\n\n# 创建特征重要性数据框\nfeature_importance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': feature_importances\n}).sort_values(by=\"Importance\", ascending=False)\nfeature_importance_df\n","outputs":[{"output_type":"execute_result","data":{"text/plain":"    Feature  Importance\n8     整流平均值    0.137515\n13     低频能量    0.106784\n7       均方根    0.106127\n20  低频奇异值特征    0.102625\n17     频带能量    0.089164\n12     高频能量    0.086777\n19  高频奇异值特征    0.084217\n2       平均值    0.054439\n15     均方频率    0.039433\n16     频率方差    0.031384\n14     重心频率    0.031075\n18    频率标准差    0.030462\n4       峰峰值    0.019554\n9      波形因子    0.019417\n1       最小值    0.013601\n3        峰值    0.011957\n5        峭度    0.011324\n0       最大值    0.009219\n11     裕度因子    0.006309\n10     峰值因子    0.004594\n6        偏度    0.004023","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>整流平均值</td>\n      <td>0.137515</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>低频能量</td>\n      <td>0.106784</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>均方根</td>\n      <td>0.106127</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>低频奇异值特征</td>\n      <td>0.102625</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>频带能量</td>\n      <td>0.089164</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>高频能量</td>\n      <td>0.086777</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>高频奇异值特征</td>\n      <td>0.084217</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>平均值</td>\n      <td>0.054439</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>均方频率</td>\n      <td>0.039433</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>频率方差</td>\n      <td>0.031384</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>重心频率</td>\n      <td>0.031075</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>频率标准差</td>\n      <td>0.030462</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>峰峰值</td>\n      <td>0.019554</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>波形因子</td>\n      <td>0.019417</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>最小值</td>\n      <td>0.013601</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>峰值</td>\n      <td>0.011957</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>峭度</td>\n      <td>0.011324</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>最大值</td>\n      <td>0.009219</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>裕度因子</td>\n      <td>0.006309</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>峰值因子</td>\n      <td>0.004594</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>偏度</td>\n      <td>0.004023</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":5}],"execution_count":5},{"cell_type":"code","metadata":{"id":"9393885A4E2D40D9B1082C09E37E878A","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"plt.figure(figsize=(10, 8))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\nplt.title(\"Random Forest Feature Importances\")\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Features\")\nplt.show()","outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x576 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/9393885A4E2D40D9B1082C09E37E878A/swyzg7zciu.png\">"},"metadata":{"needs_background":"light"}}],"execution_count":6},{"cell_type":"markdown","metadata":{"id":"CCC11721E1BC44809D64AA8EE7044786","notebookId":"6836f6e35479c3a76ac12206","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 改进HHO算法"},{"cell_type":"code","metadata":{"id":"BF285A0AF3734DF09AFC7CB15F765B60","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import numpy as np\n\nclass HarrisHawkOptimization:\n    def __init__(self, fitness_function, n_hawks, dim, max_iter, lb, ub):\n        \"\"\"\n        Harris Hawk Optimization (HHO) Algorithm\n\n        Parameters:\n            fitness_function (callable): 要优化的目标函数，返回 (score, params)\n            n_hawks (int): 鹰群数量\n            dim (int): 参数维度\n            max_iter (int): 最大迭代次数\n            lb (list or np.ndarray): 每个参数的下界\n            ub (list or np.ndarray): 每个参数的上界\n        \"\"\"\n        self.fitness_function = fitness_function\n        self.n_hawks = n_hawks\n        self.dim = dim\n        self.max_iter = max_iter\n        self.lb = np.array(lb)\n        self.ub = np.array(ub)\n        self.stagnation_count = 0\n\n    def optimize(self, verbose=True):\n        hawks = np.random.uniform(self.lb, self.ub, (self.n_hawks, self.dim))\n        fitness = np.zeros(self.n_hawks)\n        params_list = [None] * self.n_hawks\n\n        for i in range(self.n_hawks):\n            fitness[i], params_list[i] = self.fitness_function(hawks[i])\n\n        best_idx = np.argmin(fitness)\n        rabbit_pos = hawks[best_idx].copy()\n        rabbit_energy = fitness[best_idx]\n        best_params = params_list[best_idx]\n\n        global_best_pos = rabbit_pos.copy()\n        global_best_energy = rabbit_energy\n        global_best_params = best_params\n\n        last_rabbit_energy = rabbit_energy\n\n        for t in range(self.max_iter):\n            E0 = 2 * np.random.random() - 1\n            E = 2 * E0 * (1 - (t / self.max_iter) ** 2)\n\n            stagnation_limit = int(2 + 3 * (t / self.max_iter))\n\n            for i in range(self.n_hawks):\n                if abs(E) >= 1:\n                    if np.random.random() < 0.5:\n                        r_idx = np.random.randint(self.n_hawks)\n                        hawks[i] = hawks[r_idx] - np.random.random() * abs(\n                            hawks[r_idx] - 2 * np.random.random() * hawks[i])\n                    else:\n                        hawks[i] = (rabbit_pos - hawks.mean(0)) * np.random.random() + self.lb + \\\n                                   np.random.random() * (self.ub - self.lb)\n                else:\n                    r = np.random.random()\n                    J = 2 * (1 - np.random.random())\n                    if r >= 0.5 and abs(E) >= 0.5:\n                        hawks[i] = rabbit_pos - E * abs(rabbit_pos - hawks[i])\n                    elif r >= 0.5 and abs(E) < 0.5:\n                        hawks[i] = rabbit_pos - E * abs(2 * rabbit_pos - hawks[i])\n                    elif r < 0.5 and abs(E) >= 0.5:\n                        hawks[i] = rabbit_pos - E * abs(J * rabbit_pos - hawks[i])\n                    else:\n                        hawks[i] = rabbit_pos - E * abs(J * rabbit_pos - hawks[i])\n\n                hawks[i] = np.clip(hawks[i], self.lb, self.ub)\n\n                curr_fitness, curr_params = self.fitness_function(hawks[i])\n                if curr_fitness < fitness[i]:\n                    fitness[i] = curr_fitness\n                    params_list[i] = curr_params\n                if curr_fitness < rabbit_energy:\n                    rabbit_energy = curr_fitness\n                    rabbit_pos = hawks[i].copy()\n                    best_params = curr_params\n\n                if curr_fitness < global_best_energy:\n                    global_best_energy = curr_fitness\n                    global_best_pos = hawks[i].copy()\n                    global_best_params = curr_params\n\n            improvement = abs(last_rabbit_energy - rabbit_energy)\n            if improvement < 1e-6:\n                self.stagnation_count += 1\n                if self.stagnation_count >= stagnation_limit:\n                    perturbation_scale = 0.1 * (1 - t / self.max_iter)\n                    rabbit_pos += np.random.uniform(-perturbation_scale, perturbation_scale, self.dim) * (\n                        self.ub - self.lb)\n                    rabbit_pos = np.clip(rabbit_pos, self.lb, self.ub)\n                    rabbit_energy, best_params = self.fitness_function(rabbit_pos)\n\n                    if rabbit_energy < global_best_energy:\n                        global_best_energy = rabbit_energy\n                        global_best_pos = rabbit_pos.copy()\n                        global_best_params = best_params\n                    self.stagnation_count = 0\n            else:\n                self.stagnation_count = 0\n\n            last_rabbit_energy = rabbit_energy\n\n            if verbose:\n                print(f\"\\n迭代次数{t + 1}/{self.max_iter}\")\n                print(f\"当前准确率: {-rabbit_energy:.4f}\")\n                print(f\"全局最佳准确率: {-global_best_energy:.4f}\")\n                print(f\"最优参数: {best_params}\")\n\n        return global_best_pos, -global_best_energy, global_best_params\n","outputs":[],"execution_count":8},{"cell_type":"markdown","metadata":{"id":"A13C8F61000C44B5A9F7CBAF4A873E92","notebookId":"6836f6e35479c3a76ac12206","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## BP神经网络(建议到本地IDE去跑)"},{"cell_type":"code","metadata":{"id":"8F2C5BF9507F4B2CBE4360446C49DC57","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom pylab import mpl\nfrom hho import HarrisHawkOptimization\n\n# 设置中文显示\nmpl.rcParams['font.sans-serif'] = ['STZhongsong']\nmpl.rcParams['axes.unicode_minus'] = False\n\n# 读取数据\nfile_path = \"features_reconstructed_simplified_df.csv\"\ndf = pd.read_csv(file_path)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\nselected_features = [\"整流平均值\", \"均方根\", \"低频能量\", \"低频奇异值特征\", \"高频能量\", \"频带能量\", \"平均值\", \"均方频率\", \"重心频率\"]\nX = df[selected_features]\ny = df[\"故障类别\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef fitness_function(params):\n    hidden1, hidden2, learning_rate = int(params[0]), int(params[1]), params[2]\n    hidden1 = max(10, min(hidden1, 200))  # 限制范围\n    hidden2 = max(5, min(hidden2, 200))\n    learning_rate = round(max(0.0001, min(learning_rate, 0.1)), 9)\n\n    clf = MLPClassifier(\n        hidden_layer_sizes=(hidden1, hidden2),\n        activation='relu',\n        solver='adam',\n        learning_rate_init=learning_rate,\n        max_iter=2000,\n        random_state=42,\n        early_stopping=True,\n        validation_fraction=0.1\n    )\n    clf.fit(X_train_scaled, y_train)\n    y_pred = clf.predict(X_test_scaled)\n    acc = accuracy_score(y_test, y_pred)\n    return -acc, (hidden1, hidden2, learning_rate)  # 负数用于最小化目标\n\n# 设置搜索边界\nlb = [50, 50, 0.001]\nub = [200, 200, 0.1]\n\n# 初始化 HHO\nhho = HarrisHawkOptimization(\n    fitness_function=fitness_function,\n    n_hawks=20,\n    dim=3,\n    max_iter=100,\n    lb=lb,\n    ub=ub\n)\n\n# 执行优化\n_, best_accuracy, best_params = hho.optimize()\n\nhidden1, hidden2, learning_rate = best_params\n\nfinal_model = MLPClassifier(\n    hidden_layer_sizes=(hidden1, hidden2),\n    activation='relu',\n    solver='adam',\n    learning_rate_init=learning_rate,\n    max_iter=2000,\n    random_state=42,\n    early_stopping=True,\n    validation_fraction=0.1\n)\n\nfinal_model.fit(X_train_scaled, y_train)\ny_pred = final_model.predict(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"\\n=== 优化完成 ===\")\nprint(f\"最佳参数 - 隐藏层1: {hidden1}, 隐藏层2: {hidden2}, 学习率: {learning_rate}\")\nprint(f\"最终准确率: {accuracy:.4f}\")\nprint(\"分类报告:\")\nprint(report)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=final_model.classes_,\n            yticklabels=final_model.classes_)\nplt.xlabel(\"预测标签\")\nplt.ylabel(\"真实标签\")\nplt.title(\"HHO-BP 神经网络 - 故障分类混淆矩阵（优化后）\")\nplt.show()","outputs":[],"execution_count":10},{"cell_type":"markdown","metadata":{"id":"C947503CF4AA4B2090F2AAA7A6B01852","notebookId":"6836f6e35479c3a76ac12206","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"# 深度学习部分的数据"},{"cell_type":"code","metadata":{"id":"48ECACE2C2A74C308ACA3070816EE5D2","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install pyts natsort -i https://pypi.tuna.tsinghua.edu.cn/simple some-package","outputs":[{"output_type":"stream","name":"stdout","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting pyts\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b3/e3/da2042a20782b105631abe273ca5fef4390e7bdb6f5377c596891262437b/pyts-0.13.0-py3-none-any.whl (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting natsort\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ef/82/7a9d0550484a62c6da82858ee9419f3dd1ccc9aa1c26a1e43da3ecd20b0d/natsort-8.4.0-py3-none-any.whl (38 kB)\nCollecting some-package\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/a2/d318a685319c3801db1ae0002fc8e095663a55546c62a6e30d9d0fc3289b/some-package-0.1.zip (2.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.11/site-packages (from pyts) (1.26.4)\nRequirement already satisfied: scipy>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from pyts) (1.12.0)\nRequirement already satisfied: scikit-learn>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from pyts) (1.4.1.post1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from pyts) (1.3.2)\nRequirement already satisfied: numba>=0.55.2 in /opt/conda/lib/python3.11/site-packages (from pyts) (0.59.1)\nRequirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.55.2->pyts) (0.42.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=1.2.0->pyts) (3.4.0)\nBuilding wheels for collected packages: some-package\n  Building wheel for some-package (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for some-package: filename=some_package-0.1-py3-none-any.whl size=1422 sha256=2eb5e880f29a1ed8732433f7ef100fb33331a7c54379c60f6af7d0b5b7e83686\n  Stored in directory: /home/mw/.cache/pip/wheels/d7/e3/6a/7280c9d6d655157615c0306fb13fc466d4fe8b209e96f19722\nSuccessfully built some-package\nInstalling collected packages: some-package, natsort, pyts\nSuccessfully installed natsort-8.4.0 pyts-0.13.0 some-package-0.1\n"}],"execution_count":2},{"cell_type":"markdown","metadata":{"id":"A9B28869C18F45CB8D1798BC20193BE8","notebookId":"6836f6e35479c3a76ac12206","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 格拉姆角场转化"},{"cell_type":"code","metadata":{"id":"8E766C9884694E518F2A975ADBA89A26","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyts.image import GramianAngularField\nfrom pyts.approximation import PiecewiseAggregateApproximation\n\n# === 1. 参数配置 ===\nIMAGE_SIZE = 625  # 调整为与MATLAB生成的图像尺寸一致\nSAMPLE_RANGE = (-1, 1)  # 归一化范围\nCOLORMAP = 'jet'  # MATLAB默认colormap\nDPI = 100  # 分辨率控制\nPAA_SIZE = 150    # PAA降维后的时间步数\n\n# === 2. 加载数据 ===\ndata_2 = pd.read_csv('/home/mw/project/data_2.csv')\ndata = data_2.iloc[:, :-1].values  # 去掉最后一列\n\n# === 3. 创建保存路径 ===\nsave_path_gadf = '格拉姆角场差'\nos.makedirs(save_path_gadf, exist_ok=True)\n\n# === 4. 初始化PAA和GAF转换器 ===\npaa_transformer = PiecewiseAggregateApproximation(window_size=None, output_size=PAA_SIZE)\n\ngadf_transformer = GramianAngularField(\n    image_size=100,\n    method='difference',\n    sample_range=SAMPLE_RANGE,\n    overlapping=False\n)\n\n# === 5. 生成并保存图像 ===\nfor i, sample in enumerate(data):\n    # -- 应用PAA降维 --\n    sample_paa = paa_transformer.transform(sample.reshape(1, -1))\n    \n    # -- 转换为GADF --\n    gadf = gadf_transformer.fit_transform(sample_paa)[0]\n\n    # -- GADF图像 --\n    fig = plt.figure(figsize=(IMAGE_SIZE / DPI, IMAGE_SIZE / DPI), dpi=DPI)  # 将图像大小调整为6.25x6.25英寸，确保625像素\n    plt.imshow(gadf, cmap=COLORMAP, origin='lower', aspect='auto')\n    plt.axis('off')  # 去掉坐标轴\n    plt.gca().set_position([0, 0, 1, 1])  # 去掉边框，填满整个画布\n    plt.savefig(os.path.join(save_path_gadf, f\"{i+1:04d}.jpg\"),\n                bbox_inches='tight',\n                pad_inches=0,\n                dpi=DPI)\n    plt.close()\n\nprint(\"所有图像生成并保存完成！\")\n","outputs":[{"output_type":"stream","name":"stdout","text":"所有图像生成并保存完成！\n"}],"execution_count":14},{"cell_type":"code","metadata":{"id":"3BA39C39A4924FD9B3C3C1F99095E636","notebookId":"6836f6e35479c3a76ac12206","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport glob\nfrom natsort import natsorted\nimport warnings\n\ndef process_gaf_images(\n    input_folder,           # 要处理的文件夹路径\n    data_path,              # CSV数据文件路径\n    output_path='output.npz',  # 输出文件完整路径\n    target_size=(64, 64),   # 目标图像尺寸\n    save_format='npz',      # 保存格式: 'npz' 或 'mat'\n    verbose=True            # 是否打印进度信息\n):\n    # 参数验证\n    assert save_format in ['npz', 'mat'], \"仅支持 npz/mat 格式\"\n    \n    # 创建输出目录（如果路径包含目录）\n    output_dir = os.path.dirname(output_path)\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n\n    # 加载标签数据\n    try:\n        labels = pd.read_csv(data_path).iloc[:, -1].values.flatten()\n    except Exception as e:\n        raise ValueError(f\"加载CSV数据失败: {str(e)}\")\n\n    # 获取并排序图像文件\n    image_files = natsorted(glob.glob(os.path.join(input_folder, '*.jpg')))\n    if not image_files:\n        warnings.warn(f\"文件夹 {input_folder} 中没有找到jpg文件\", UserWarning)\n        return\n\n    # 检查图像数量与标签数量是否一致\n    assert len(image_files) == len(labels), (\n        f\"图像数量({len(image_files)})与标签数量({len(labels)})不匹配\"\n    )\n\n    # 处理图像\n    images = []\n    for idx, img_path in enumerate(image_files):\n        with Image.open(img_path) as img:\n            if img.mode != 'RGB':\n                img = img.convert('RGB')\n            img_array = np.array(img.resize(target_size), dtype=np.float32) / 255.0\n            images.append(img_array)\n        \n        if verbose and (idx + 1) % 100 == 0:\n            print(f\"已处理 {idx + 1}/{len(image_files)} 张图像\")\n\n    # 保存数据\n    images, labels = np.array(images), np.array(labels)\n    if save_format == 'npz':\n        np.savez(output_path, images=images, labels=labels)\n    else:\n        from scipy.io import savemat\n        savemat(output_path, {'resizeimg': images, 'labels': labels}, do_compression=True)\n\n    if verbose:\n        print(f\"\\n处理完成！保存文件: {output_path}\")\n        print(f\"最终数据形状: 图像 {images.shape}, 标签 {labels.shape}\")\n\nif __name__ == \"__main__\":\n    process_gaf_images(\n        input_folder='/home/mw/project/格拉姆角场差',\n        data_path='/home/mw/project/data_2.csv',\n        output_path='格拉姆角场差.mat',\n        target_size=(64, 64),\n        save_format='mat',\n        verbose=True\n    )","outputs":[{"output_type":"stream","name":"stdout","text":"已处理 100/1600 张图像\n已处理 200/1600 张图像\n已处理 300/1600 张图像\n已处理 400/1600 张图像\n已处理 500/1600 张图像\n已处理 600/1600 张图像\n已处理 700/1600 张图像\n已处理 800/1600 张图像\n已处理 900/1600 张图像\n已处理 1000/1600 张图像\n已处理 1100/1600 张图像\n已处理 1200/1600 张图像\n已处理 1300/1600 张图像\n已处理 1400/1600 张图像\n已处理 1500/1600 张图像\n已处理 1600/1600 张图像\n\n处理完成！保存文件: 格拉姆角场差.mat\n最终数据形状: 图像 (1600, 64, 64, 3), 标签 (1600,)\n"}],"execution_count":3}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}